# An unique identifier for the head node and workers of this cluster.
cluster_name: tali-ray

# The minimum number of workers nodes to launch in addition to the head
# node. This number should be >= 0.
min_workers: 0

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers.
max_workers: 1

# The initial number of worker nodes to launch in addition to the head
# node. When the cluster is first brought up (or when it is refreshed with a
# subsequent `ray up`) this number of nodes will be started.
initial_workers: 0
upscaling_speed: 1.0
# The autoscaler will scale up the cluster to this target fraction of resource
# usage. For example, if a cluster of 10 nodes is 100% busy and
# target_utilization is 0.8, it would resize the cluster to 13. This fraction
# can be decreased to increase the aggressiveness of upscaling.
# This value must be less than 1.0 for scaling to happen.

#docker:
#  image: "rayproject/ray-ml:latest-gpu" # You can change this to latest-cpu if you don't need GPU support and want a faster startup
#    # image: rayproject/ray:latest-gpu   # use this one if you don't need ML dependencies, it's faster to pull
#  container_name: "ray_ml_container"
#  # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image
#  # if no cached version is present.
#  pull_before_run: True
#  run_options:  # Extra options to pass into "docker run"
#    - --ulimit nofile=65536:65536
#
#  # Example of running a GPU head with CPU workers
#  # head_image: "rayproject/ray-ml:latest-gpu"
#  # Allow Ray to automatically detect GPUs
#
#  # worker_image: "rayproject/ray-ml:latest-cpu"
#  # worker_run_options: []

target_utilization_fraction: 0.8

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 60

# Cloud-provider specific configuration.
provider:
    type: gcp
    region: us-central1
    availability_zone: us-central1-f
    project_id: tali-multi-modal # Globally unique project id
    cache_stopped_nodes: False

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: evolvingfungus
# By default Ray creates a new private keypair, but you can also use your own.
# If you do so, make sure to also set "KeyName" in the head and worker node
# configurations below. This requires that you have added the key into the
# project wide meta-data.
#    ssh_private_key: /path/to/your/key.pem

# Provider-specific config for the head node, e.g. instance type. By default
# Ray will auto-configure unspecified fields such as subnets and ssh-keys.
# For more documentation on available fields, see:
# https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

# pytorch-latest-gpu-v20211219-ubuntu-2004
# deeplearning-platform-release
# pytorch-latest-gpu-ubuntu-2004

available_node_types:
    ray_head_default:
        # The resources provided by this node type.
#        canIpForward: false
#        confidentialInstanceConfig: {"enableConfidentialCompute": false}
        resources: {"CPU": 2, "GPU": 0}
        # Provider-specific config for the head node, e.g. instance type. By default
        # Ray will auto-configure unspecified fields such as subnets and ssh-keys.
        # For more documentation on available fields, see:
        # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

        node_config:
            machineType: projects/tali-multi-modal/zones/us-central1-f/machineTypes/n2-standard-2
            disks:
              - boot: true
                autoDelete: true
                type: PERSISTENT
                mode: READ_WRITE
                initializeParams:
                  diskSizeGb: 200
                  disktype: projects/tali-multi-modal/zones/us-central1-f/diskTypes/pd-standard
                  # See https://cloud.google.com/compute/docs/images for more images
                  sourceImage: projects/tali-multi-modal/global/images/tali-base-gpu-image-v-2-0
#              - boot: false
#                autoDelete: true
#                type: PERSISTENT
#                mode: READ_WRITE
#                initializeParams:
#                  diskSizeGb: 3500
#                  disktype: projects/tali-multi-modal/zones/us-central1-f/diskTypes/pd-ssd
#                  # See https://cloud.google.com/compute/docs/images for more images
#                  sourceImage: projects/tali-multi-modal/global/images/tali-dataset-v2-6-us-central1-full-compact
            scheduling:
              - preemptible: false
              - automaticRestart: true
              - onHostMaintenance: TERMINATE
          # Additional options can be found in in the compute docs at
            # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

            # If the network interface is specified as below in both head and worker
            # nodes, the manual network config is used.  Otherwise an existing subnet is
            # used.  To use a shared subnet, ask the subnet owner to grant permission
            # for 'compute.subnetworks.use' to the ray autoscaler account...
            # networkInterfaces:
            #   - kind: compute#networkInterface
            #     subnetwork: path/to/subnet
            #     aliasIpRanges: []
    ray_worker_small:
        # The minimum number of worker nodes of this type to launch.
        # This number should be >= 0.
        min_workers: 0
        # The maximum number of worker nodes of this type to launch.
        # This takes precedence over min_workers.
        max_workers: 32
        # The resources provided by this node type.
        resources: {"CPU": 12, "GPU": 1}
        # Provider-specific config for the head node, e.g. instance type. By default
        # Ray will auto-configure unspecified fields such as subnets and ssh-keys.
        # For more documentation on available fields, see:
        # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
        node_config:
          machineType: projects/tali-multi-modal/zones/us-central1-f/machineTypes/a2-highgpu-1g
          disks:
            - boot: true
              autoDelete: true
              type: PERSISTENT
              mode: READ_WRITE
              initializeParams:
                diskSizeGb: 200
                disktype: projects/tali-multi-modal/zones/us-central1-f/diskTypes/pd-standard
                # See https://cloud.google.com/compute/docs/images for more images
                sourceImage: projects/tali-multi-modal/global/images/tali-base-gpu-image-v-2-0
            - boot: false
              autoDelete: true
              type: PERSISTENT
              mode: READ_WRITE
              initializeParams:
                diskSizeGb: 3500
                disktype: projects/tali-multi-modal/zones/us-central1-f/diskTypes/pd-ssd
                # See https://cloud.google.com/compute/docs/images for more images
                sourceImage: projects/tali-multi-modal/global/images/tali-dataset-v2-6-us-central1-full-compact
          scheduling:
            - preemptible: true
            - automaticRestart: true
            - onHostMaintenance: TERMINATE
          serviceAccounts:
            - {
              "email": "tali-multi-modal@tali-multi-modal.iam.gserviceaccount.com",
              "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
              ]
            }
            # Un-Comment this to launch workers with the Service Account of the Head Node
            # serviceAccounts:
            # - email: ray-autoscaler-sa-v1@<project_id>.iam.gserviceaccount.com
            #   scopes:
            #   - https://www.googleapis.com/auth/cloud-platform
    ray_worker_medium:
      # The minimum number of worker nodes of this type to launch.
      # This number should be >= 0.
      min_workers: 0
      # The maximum number of worker nodes of this type to launch.
      # This takes precedence over min_workers.
      max_workers: 16
      # The resources provided by this node type.
      resources: {"CPU": 24, "GPU": 2}
      # Provider-specific config for the head node, e.g. instance type. By default
      # Ray will auto-configure unspecified fields such as subnets and ssh-keys.
      # For more documentation on available fields, see:
      # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
      node_config:
        machineType: projects/tali-multi-modal/zones/us-central1-f/machineTypes/a2-highgpu-2g
        disks:
          - boot: true
            autoDelete: true
            type: PERSISTENT
            mode: READ_WRITE
            initializeParams:
              diskSizeGb: 200
              disktype: projects/tali-multi-modal/zones/us-central1-f/diskTypes/pd-standard
              # See https://cloud.google.com/compute/docs/images for more images
              sourceImage: projects/tali-multi-modal/global/images/tali-base-gpu-image-v-2-0
          - boot: false
            autoDelete: true
            type: PERSISTENT
            mode: READ_WRITE
            initializeParams:
              diskSizeGb: 3500
              disktype: projects/tali-multi-modal/zones/us-central1-f/diskTypes/pd-ssd
              # See https://cloud.google.com/compute/docs/images for more images
              sourceImage: projects/tali-multi-modal/global/images/tali-dataset-v2-6-us-central1-full-compact
        scheduling:
          - preemptible: true
          - automaticRestart: true
          - onHostMaintenance: TERMINATE
        serviceAccounts:
          - {
            "email": "tali-multi-modal@tali-multi-modal.iam.gserviceaccount.com",
            "scopes": [
                "https://www.googleapis.com/auth/cloud-platform"
            ]
          }

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
    "/home/evolvingfungus/current_research_forge/": "/home/evolvingfungus/current_research_forge/",
}

cluster_synced_files: []

# List of commands that will be run before `setup_commands`. If docker is
# enabled, these commands will run outside the container and before docker
# is setup.

rsync_exclude:
    - "**/.git"
    - "**/.git/**"

# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for
# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided
# as a value, the behavior will match git's behavior for finding and using .gitignore files.
rsync_filter: []

initialization_commands: []

# List of shell commands to run to set up nodes.
setup_commands:
  - bash ${HOME}/current_research_forge/TALI/setup_scripts/compact_setup_poetry.sh
  - bash ${HOME}/current_research_forge/TALI/setup_scripts/setup_base_experiment_disk.sh
  - rsync -ua --progress ${HOME}/current_research_forge/TALI/tali/configs/ ${HOME}/configs/

# Custom commands that will be run on the head node after common setup.
head_setup_commands:
    # Allow Ray applications to automatically connect to the cluster.
#    - bash ${HOME}/current_research_forge/TALI/setup_scripts/setup_tali_dataset_disk.sh
  - echo 'export RAY_ADDRESS="localhost:6379"' >> ~/.bashrc

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands:
  - bash ${HOME}/current_research_forge/TALI/setup_scripts/setup_tali_dataset_disk.sh

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - >-
      ulimit -n 65536;
      ray start
      --head
      --port=6379
      --object-manager-port=8076
      --verbose
      --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - >-
      ulimit -n 65536;
      ray start
      --address=$RAY_HEAD_IP:6379
      --object-manager-port=8076